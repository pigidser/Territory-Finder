{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utilities import *\n",
    "\n",
    "import logging\n",
    "import os, sys\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import openpyxl\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Font, Alignment, PatternFill\n",
    "\n",
    "class TerritoryFinder(object):\n",
    "\n",
    "    def __init__(self, coord_file, report_file, output_file, samples_threshold=2):\n",
    "        \"\"\" Class initialization, logging set-up, checking input files \"\"\"\n",
    "        # input and output files\n",
    "        self.coord_file, self.report_file, self.output_file = coord_file, report_file, output_file\n",
    "        # Выборка не сбалансированная, используем class_weight='balanced', n_estimators=40\n",
    "        self.model = RandomForestClassifier(class_weight='balanced', n_estimators=40, random_state=42, n_jobs=-1, warm_start=False)\n",
    "        self.df = pd.DataFrame()\n",
    "        self.X_enc_train, self.y_enc_train, self.X_enc_pred = None, None, None\n",
    "        self.set_up_logging()\n",
    "        self.check_env()\n",
    "        # Порог для исключения классов\n",
    "        self.samples_threshold = samples_threshold\n",
    "        self.log.debug(\"TerritoryFinder class initialized\")\n",
    "\n",
    "    def set_up_logging(self):\n",
    "        \"\"\" Set up logging\n",
    "        \n",
    "        \"\"\"\n",
    "        os.makedirs('logs', exist_ok=True)\n",
    "        self.log = logging.getLogger(__name__)\n",
    "        self.log.setLevel(logging.DEBUG)\n",
    "        formatter = logging.Formatter(fmt=\"%(asctime)s %(levelname)s: %(message)s\", datefmt=\"%Y-%m-%d - %H:%M:%S\")\n",
    "        sh = logging.StreamHandler(sys.stdout)\n",
    "        sh.setLevel(logging.DEBUG)\n",
    "        sh.setFormatter(formatter)\n",
    "        fh = logging.FileHandler(u\"./logs/territory_finder.log\", \"w\")\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        fh.setFormatter(formatter)\n",
    "        self.log.addHandler(sh)\n",
    "        self.log.addHandler(fh)\n",
    "    \n",
    "    def check_env(self):\n",
    "        if not os.path.isfile(self.coord_file):\n",
    "            self.log.error(f\"File '{self.coord_file}' not found. Please place it in a folder with this program\")\n",
    "            raise Exception\n",
    "        if not os.path.isfile(self.report_file):\n",
    "            self.log.error(f\"File '{self.report_file}' not found. Please place it in a folder with this program\")\n",
    "            raise Exception\n",
    "        self.log.debug(f\"Input files were found\")\n",
    "\n",
    "    def restore_coordinates(self):\n",
    "        \"\"\" Find coordinates for an outlet by its neighbors \"\"\"\n",
    "\n",
    "        self.df['Latitude'].replace(0, np.NaN, inplace=True)\n",
    "        self.df['Longitude'].replace(0, np.NaN, inplace=True)\n",
    "        self.df['isCoord'] = ~( (self.df['Latitude'].isna()) | (self.df['Longitude'].isna()) )\n",
    "\n",
    "        kladr_lat_grouped = self.df[self.df['isCoord']==1].groupby(['Kladr_level_1','Kladr_level_2','Kladr_level_3','Kladr_level_4']).Latitude.mean()\n",
    "        kladr_lon_grouped = self.df[self.df['isCoord']==1].groupby(['Kladr_level_1','Kladr_level_2','Kladr_level_3','Kladr_level_4']).Longitude.mean()\n",
    "\n",
    "        def get_avg_coordinate(row, kladr_grouped):\n",
    "            \"\"\"\n",
    "            Вернуть среднюю координату населенного пункта, области, региона или страны. Используем функции\n",
    "            multiindex.isin().any(), чтобы проверить, что в Series имеется индекс для всех 4-х уровней\n",
    "            и вернуть значение. В случае отсутствия индекса, отрубить последний уровень в индексе и проверить\n",
    "            индекс для 3-х уровней и т.д.\n",
    "\n",
    "            Parameters:\n",
    "            row (Series): ['Kladr_level_1','Kladr_level_2','Kladr_level_3','Kladr_level_4'] для которых нужно\n",
    "                получить координату\n",
    "            kladr_grouped (Series): с мультииндексом (['Kladr_level_1','Kladr_level_2','Kladr_level_3','Kladr_level_4'])\n",
    "                который содержит значения координаты для 4-х уровней из адресного классификатора\n",
    "                \n",
    "            Returns:\n",
    "            float: Координата\n",
    "\n",
    "            \"\"\"\n",
    "            try:\n",
    "                if kladr_grouped.index \\\n",
    "                        .isin([(row['Kladr_level_1'],row['Kladr_level_2'],row['Kladr_level_3'],row['Kladr_level_4'])]).any():\n",
    "                    return kladr_grouped[row['Kladr_level_1'],row['Kladr_level_2'],row['Kladr_level_3'],row['Kladr_level_4']]\n",
    "                elif kladr_grouped.index.droplevel(['Kladr_level_4']) \\\n",
    "                        .isin([(row['Kladr_level_1'],row['Kladr_level_2'],row['Kladr_level_3'])]).any():\n",
    "                    return kladr_grouped[row['Kladr_level_1'],row['Kladr_level_2'],row['Kladr_level_3']].mean()\n",
    "                elif kladr_grouped.index.droplevel(['Kladr_level_3','Kladr_level_4']) \\\n",
    "                        .isin([(row['Kladr_level_1'],row['Kladr_level_2'])]).any():\n",
    "                    return kladr_grouped[row['Kladr_level_1'],row['Kladr_level_2']].mean()\n",
    "                elif kladr_grouped.index.droplevel(['Kladr_level_2','Kladr_level_3','Kladr_level_4']) \\\n",
    "                        .isin([(row['Kladr_level_1'])]).any():\n",
    "                    return kladr_grouped[row['Kladr_level_1']].mean()\n",
    "                else:\n",
    "                    return 0\n",
    "            except:\n",
    "                print(row['Kladr_level_1'],row['Kladr_level_2'],row['Kladr_level_3'],row['Kladr_level_4'])\n",
    "                raise KeyError\n",
    "\n",
    "        self.df.loc[self.df['isCoord']==0,'Latitude'] = \\\n",
    "            self.df.loc[self.df['isCoord']==0][['SWE_Store_Key','Kladr_level_1','Kladr_level_2','Kladr_level_3','Kladr_level_4']].apply( \\\n",
    "                get_avg_coordinate, args=(kladr_lat_grouped,), axis=1)\n",
    "\n",
    "        self.df.loc[self.df['isCoord']==0,'Longitude'] = \\\n",
    "            self.df.loc[self.df['isCoord']==0][['SWE_Store_Key','Kladr_level_1','Kladr_level_2','Kladr_level_3','Kladr_level_4']].apply( \\\n",
    "                get_avg_coordinate, args=(kladr_lon_grouped,), axis=1)\n",
    "\n",
    "        self.df.drop(['Kladr_level_1','Kladr_level_2','Kladr_level_3','Kladr_level_4','Kladr_level_5'], axis=1, inplace=True)\n",
    "\n",
    "    def load_data(self):          \n",
    "        \"\"\" Load and transform data \"\"\"\n",
    "\n",
    "        self.log.info(f\"Loading coordinates...\")\n",
    "        df_coor = pd.read_excel(self.coord_file, nrows=1000)\n",
    "        self.log.debug(f\"Rows in {self.coord_file}: {df_coor.shape[0]}\")\n",
    "        df_coor.columns = ['SWE_Store_Key','Latitude','Longitude']\n",
    "        # cleansing from invalid coordinates\n",
    "        df_coor = df_coor[df_coor['Latitude']!=0]\n",
    "        df_coor = df_coor[(df_coor['Latitude']>40)&(df_coor['Latitude']<82)]\n",
    "        df_coor = df_coor[((df_coor['Longitude']>=10)&(df_coor['Longitude']<180)) | \\\n",
    "            ((df_coor['Longitude']>=-180)&(df_coor['Longitude']<-160))]\n",
    "\n",
    "        # check if outlets are duplicated\n",
    "        if df_coor.SWE_Store_Key.value_counts().values[0] > 1:\n",
    "            self.log.error(f\"Found duplicated codes of outlets in '{self.coord_file}!\")\n",
    "            raise Exception\n",
    "\n",
    "        self.log.info(f\"Loading report file...\")\n",
    "        df_terr = pd.read_excel(self.report_file, skiprows=1)\n",
    "        self.log.debug(f\"Rows in {self.report_file}: {df_terr.shape[0]}\")\n",
    "        # rename fields\n",
    "        df_terr.columns = ['Region','Distrib','Office','FFDSL','TSE_MTDE',\n",
    "            'Level_Torg_Region1','Level_Torg_Region2','Filial_Name',\n",
    "            'Filial_Ship_To','Chain_Type','Chain_Name','Chain_Id',\n",
    "            'Chain_Chain_Tier_MWC','Chain_Chain_Sub_Tier_MWC','SWE_Store_Key',\n",
    "            'Store_Status','Store_Status_NOW','Outlet_Name','Channel_Name_2018',\n",
    "            'Outlet_Type_2018','Trade_Structure','From_Dc',\n",
    "            'Segment_MWC_Segment_Name','Cluster_MWC','Kladr_level_1',\n",
    "            'Kladr_level_2','Kladr_level_3','Kladr_level_4','Kladr_level_5',\n",
    "            'LSV_WWY','LSV_CHOCO','LSV_MWC','Covering_Outlet_id',\n",
    "            'General_Duplicate','Ship_To_Visited','Filial_Visited',\n",
    "            'Ship_to_Name_TO_BE','Region_loaded_RSS',\n",
    "            'MW_Ship_to_TO_BE_Name_loaded_RSS',\n",
    "            'MW_Ship_to_TO_BE_loaded_RSS',\n",
    "            'CH_Ship_to_TO_BE_Name_loaded_RSS',\n",
    "            'CH_Ship_to_TO_BE_loaded_RSS',\n",
    "            'WR_Ship_to_TO_BE_Name_loaded_RSS',\n",
    "            'WR_Ship_to_TO_BE_loaded_RSS','Ship_to_Code_TO_BE',\n",
    "            'DC','Changed',\n",
    "            'Change_Period','Region_Last_Future_Ship_to',\n",
    "            'Last_Future_ship_to_Name', 'Last_Future_ship_to', 'Comment']\n",
    "\n",
    "        self.df_codes = pd.DataFrame(data=df_terr['SWE_Store_Key'],columns=['SWE_Store_Key'])\n",
    "\n",
    "        # do not take unused columns\n",
    "        df_terr = df_terr[['SWE_Store_Key','Region','Distrib','Office','FFDSL','TSE_MTDE','Level_Torg_Region1',\n",
    "            'Level_Torg_Region2','Filial_Name','Filial_Ship_To','Chain_Type','Chain_Id','Chain_Chain_Tier_MWC',\n",
    "            'Chain_Chain_Sub_Tier_MWC','Channel_Name_2018','Outlet_Type_2018','Trade_Structure','From_Dc',\n",
    "            'Segment_MWC_Segment_Name','Cluster_MWC','Covering_Outlet_id','General_Duplicate','Ship_To_Visited',\n",
    "            'Kladr_level_1','Kladr_level_2','Kladr_level_3','Kladr_level_4','Kladr_level_5',\n",
    "            'Region_Last_Future_Ship_to','Last_Future_ship_to_Name','Last_Future_ship_to']]\n",
    "\n",
    "        # Remove outlet-duplicates and associated fields\n",
    "        df_terr = df_terr[df_terr['General_Duplicate']!='Дубликат']\n",
    "        df_terr.drop(['Covering_Outlet_id','General_Duplicate'], axis=1, inplace=True)\n",
    "\n",
    "        self.log.info(\"Merging territories with coordinates and start preprocessing...\")\n",
    "        self.df = pd.merge(df_terr, df_coor, on='SWE_Store_Key',how='left')\n",
    "        del df_terr\n",
    "        del df_coor\n",
    "\n",
    "        self.log.info(\"Restore coordinates...\")\n",
    "        self.restore_coordinates()\n",
    "\n",
    "        self.df['isTrain'] = ~ self.df['Last_Future_ship_to'].isna()\n",
    "\n",
    "        # Last_Future_ship_to убрать внешние пробелы и преобразовать к типу str\n",
    "        # self.df['Last_Future_ship_to'] = self.df['Last_Future_ship_to'].astype(str)\n",
    "        self.df.loc[self.df['isTrain']==True,'Last_Future_ship_to'] = \\\n",
    "            self.df.loc[self.df['isTrain']==True]['Last_Future_ship_to'].apply(self.align_value)\n",
    "\n",
    "        self.df['From_Dc'] = self.df['From_Dc'].astype(int)\n",
    "        self.df['Chain_Id'] = self.df['Chain_Id'].astype(str)\n",
    "        # Установить поле как индекс, тем самым исключив его из списка признаков\n",
    "        self.df.set_index('SWE_Store_Key',inplace=True)\n",
    "        # Классы для исключения\n",
    "        self.ships_to_exclude = self.get_ships_to_exclude(self.samples_threshold)\n",
    "\n",
    "    def align_value(self, value):\n",
    "        \"\"\"\n",
    "        Избавиться от крайних символов и дублирующихся запятых.\n",
    "        Если получен список, то отсортировать по возрастанию\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            aligned = value\n",
    "            # Избавится от крайних символов и дублирующихся запятых\n",
    "            try:\n",
    "                aligned = str(int(float(aligned)))\n",
    "            except ValueError:\n",
    "                aligned = aligned.strip().replace(', ,',',').replace(',  ,',',') \\\n",
    "                    .replace(',,',',').replace(',,',',').replace(',,',',')\n",
    "                while aligned[0] not in ['0','1','2','3','4','5','6','7','8','9']:\n",
    "                    aligned = aligned[1:]\n",
    "                while aligned[-1] not in ['0','1','2','3','4','5','6','7','8','9']:\n",
    "                    aligned = aligned[:-1]\n",
    "                aligned = np.array(aligned.split(',')).astype('float').astype('int')\n",
    "#                 aligned = np.sort(aligned)\n",
    "                aligned = ','.join(aligned.astype(str))\n",
    "        except Exception as e:\n",
    "            print(f\"Возникла проблема при обработке кода {value}. Ошибка {e}\")\n",
    "            return value\n",
    "        finally:\n",
    "            return aligned\n",
    "\n",
    "    def get_ships_to_exclude(self, threshold=2):\n",
    "        \"\"\"\n",
    "        Вернуть список классов с количеством сэмплов меньшим threshold\n",
    "    \n",
    "        \"\"\"\n",
    "        if self.df.empty:\n",
    "            self.log.error(f\"Envoke the load_data method first!\")\n",
    "            raise Exception\n",
    "        if threshold < 2:\n",
    "            threshold = 2\n",
    "        ship_counts = self.df[~self.df['Last_Future_ship_to'].isna()].groupby('Last_Future_ship_to').size().to_frame()\n",
    "        ship_counts.reset_index(inplace=True)\n",
    "        ship_counts.columns = ['Last_Future_ship_to','Counts']\n",
    "        \n",
    "        return [str(item) for item in list(ship_counts['Last_Future_ship_to'][ship_counts['Counts']<threshold].values)]\n",
    "\n",
    "    def get_encoded(self):\n",
    "        \"\"\" Ordinal encoding implementation \"\"\"\n",
    "\n",
    "        # fill NaN values, not touching target variable\n",
    "        cat_features = self.df.select_dtypes(include=['object']).columns  # Categorical\n",
    "        num_features = self.df.select_dtypes(exclude=['object']).columns  # Numeric\n",
    "        self.df['Last_Future_ship_to'].replace(np.NaN, 0, inplace=True)\n",
    "        for name in cat_features:\n",
    "            self.df[name].fillna('missing', inplace=True)\n",
    "        for name in num_features:\n",
    "            self.df[name].fillna(0, inplace=True)\n",
    "        self.df['Last_Future_ship_to'].replace(0, np.NaN, inplace=True)\n",
    "\n",
    "        target = ['Last_Future_ship_to']         # Целевая переменная\n",
    "        aux_target = ['Region_Last_Future_Ship_to','Last_Future_ship_to_Name']\n",
    "        service = ['isTrain','isCoord']  # Сервисные признаки, которые отбросить\n",
    "        # Из полного списка признаков или из переданного списка признаков исключить target, aux_target & service\n",
    "        self.features = [column for column in self.df.columns \\\n",
    "                    if column not in target + aux_target + service]\n",
    "        \n",
    "        # Обучить кодировщик на полном наборе признаков\n",
    "        X = self.df[(~self.df['Last_Future_ship_to'].isin(self.ships_to_exclude))][self.features]\n",
    "        cat_features = X.select_dtypes(include=['object']).columns  # Categorical\n",
    "        num_features = X.select_dtypes(exclude=['object']).columns  # Numeric\n",
    "        self.encoder_x = OrdinalEncoder()\n",
    "        self.encoder_x.fit(X[cat_features])\n",
    "\n",
    "        X_train = self.df[(self.df['isTrain']==True)&(~self.df['Last_Future_ship_to'].isin(self.ships_to_exclude))][self.features]\n",
    "        X_cat = self.encoder_x.transform(X_train[cat_features])       # Transform cats features\n",
    "        X_num = X_train[num_features]                                   # Not transform nums features\n",
    "        self.X_enc_train = np.hstack([X_cat, X_num])                  # Join\n",
    "\n",
    "        X_pred = self.df[(self.df['isTrain']==False)&(~self.df['Last_Future_ship_to'].isin(self.ships_to_exclude))][self.features]\n",
    "        X_cat = self.encoder_x.transform(X_pred[cat_features])\n",
    "        X_num = X_pred[num_features]\n",
    "        self.X_enc_pred = np.hstack([X_cat, X_num])\n",
    "        self.log.debug(f\"Shapes: X {X.shape}, X_enc_train {self.X_enc_train.shape}, X_enc_pred {self.X_enc_pred.shape}\")\n",
    "        \n",
    "        # Transform y\n",
    "        y = self.df[(self.df['isTrain']==True)&(~self.df['Last_Future_ship_to'].isin(self.ships_to_exclude))][target]\n",
    "        self.encoder_y = LabelEncoder()\n",
    "        # y is a DataFrame, converting to 1D array\n",
    "        self.y_enc_train = self.encoder_y.fit_transform(y.values.ravel())\n",
    "        self.log.debug(f\"Shape: y_enc_train {self.y_enc_train.shape}\")\n",
    "        \n",
    "    def validate(self):\n",
    "        \"\"\" Split the dataset into the training and the validation parts for training and validation \"\"\"\n",
    "        self.get_encoded()\n",
    "        # Training-Validation split\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(self.X_enc_train, self.y_enc_train,\n",
    "            test_size=0.3, random_state=42, stratify=self.y_enc_train)\n",
    "        # Training, validation, Cross-Validation\n",
    "        t0 = time()\n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.log.debug(f\"Training finished in {time() - t0:.3f} sec.\")\n",
    "        t0 = time()\n",
    "        y_pred = self.model.predict(X_valid)\n",
    "        self.val_score = balanced_accuracy_score(y_valid, y_pred)\n",
    "        self.log.info(f\"Balanced accuracy score: {self.val_score:.3f}\")\n",
    "        self.log.debug(f\"Validation finished in {time() - t0:.1f} sec.\")\n",
    "        t0 = time()\n",
    "        val_cv_score = cross_val_score(self.model, self.X_enc_train, self.y_enc_train, cv=3, scoring='balanced_accuracy')\n",
    "        self.val_cv_score = np.array([round(item, 5) for item in val_cv_score])\n",
    "        self.log.info(f\"Cross-validation average score: {self.val_cv_score.mean():.3f}\")\n",
    "        self.log.debug(f\"Cross-validation finished in {time() - t0:.3f} sec.\")\n",
    "        # print statistics\n",
    "        self.get_statistics(X_valid, y_valid)\n",
    "\n",
    "    def get_statistics(self, X_valid, y_valid):\n",
    "        \"\"\" Print statistics \"\"\"\n",
    "        self.find_top_3(X_valid)\n",
    "        # y_valid закодирован, поэтому инвертируем как было\n",
    "        self.proba['y_valid'] = self.encoder_y.inverse_transform(y_valid)\n",
    "        self.proba['correct_1'] = self.proba.apply(lambda x: int(x.top_1_class==x.y_valid),axis=1)\n",
    "        self.proba['correct_2'] = self.proba.apply(lambda x: int(x.top_2_class==x.y_valid),axis=1)\n",
    "        self.proba['correct_3'] = self.proba.apply(lambda x: int(x.top_3_class==x.y_valid),axis=1)\n",
    "\n",
    "        # Оставим только новые столбцы с информацией по 3-м классам с наивысшей уверенностью\n",
    "        self.proba = self.proba.loc[:,'top_1_class':]\n",
    "\n",
    "        # Всего предсказаний\n",
    "        total = self.proba.shape[0]\n",
    "        # Количество верных предсказаний по классам \n",
    "        corr_cl1 = self.proba[self.proba.top_1_class==self.proba.y_valid].shape[0]\n",
    "        corr_cl2 = self.proba[self.proba.top_2_class==self.proba.y_valid].shape[0]\n",
    "        corr_cl3 = self.proba[self.proba.top_3_class==self.proba.y_valid].shape[0]\n",
    "        not_correct = total - (corr_cl1 + corr_cl2 + corr_cl3)\n",
    "        self.log.info(f\"\"\"\n",
    "        Всего предсказаний: {total}\n",
    "        Правильных предсказаний: {corr_cl1/total*100:.1f}% ({corr_cl1})\n",
    "        Предсказанных во втором варианте: {corr_cl2/total*100:.2f}% ({corr_cl2})\n",
    "        Предсказанных в третьем варианте: {corr_cl3/total*100:.3f}% ({corr_cl3})\n",
    "        Не предсказанных вообще {not_correct/total*100:.3f}% ({not_correct})\n",
    "        \"\"\")\n",
    "\n",
    "        def get_proba_info(class_num, proba_from, proba_to):\n",
    "            \"\"\"\n",
    "            Получает название класса и возвращает количество правильных, не правильных ответов, а также интервал\n",
    "            \n",
    "            \"\"\"\n",
    "            correct_num = self.proba[(self.proba[class_num+'_class']==self.proba.y_valid)& \\\n",
    "                (self.proba[class_num+'_proba']>proba_from)&(self.proba[class_num+'_proba']<=proba_to)].shape[0]\n",
    "            incorrect_num = self.proba[(self.proba[class_num+'_class']!=self.proba.y_valid)& \\\n",
    "                (self.proba[class_num+'_proba']>proba_from)&(self.proba[class_num+'_proba']<=proba_to)].shape[0]\n",
    "            return correct_num, incorrect_num, (proba_from, proba_to)\n",
    "\n",
    "        correct, incorrect, index = [], [], []\n",
    "        for edge in range(20,100,10):\n",
    "            cor, inc, ind = get_proba_info('top_1',edge/100,(edge+10)/100)\n",
    "            correct.append(cor)\n",
    "            incorrect.append(inc)\n",
    "            index.append(ind)\n",
    "            \n",
    "        a = pd.DataFrame(data={'correct':correct[::-1], 'incorrect':incorrect[::-1]}, index=index[::-1])    \n",
    "\n",
    "        # Напечатать интервальную серию с выводом информации об отношении количеств элементов в интервалах \n",
    "        top = 1\n",
    "        bottom = 0\n",
    "        rep_list = []\n",
    "        rep_list.append(f\"\\n{'Интервал':>12} {'Верных':>8} {'Неверных':>10} {'Нев./Общ.':>11}\\n\")\n",
    "        for i in range(len(a)):\n",
    "            mid = a.index[i][0]\n",
    "            s = f\"{str(a.index[i]):>12} {a.correct[i]:>8} {a.incorrect[i]:>10}\"\n",
    "            if i==0:\n",
    "                v = a.incorrect[i] / (a.incorrect[i] + a.correct[i]) * 100\n",
    "            else:\n",
    "                v = a.incorrect[:i+1].sum() / (a.incorrect[:i+1].sum() + a.correct[:i+1].sum()) * 100\n",
    "            rep_list.append(\"{0} {1:>10.2f} | интервал ({2}, {3}] кол. ошибок / общему кол. предсказаний = {4:.2f}%\\n\" \\\n",
    "                    .format(s, a.incorrect[i] / (a.incorrect[i] + a.correct[i]) * 100, mid, top, v))\n",
    "        self.log.info(''.join(rep_list))\n",
    "\n",
    "    def find_top_3(self, X_valid):\n",
    "        \"\"\" Define top 3 classes for each outlet without an answer \"\"\"\n",
    "        y_pred_proba = self.model.predict_proba(X_valid)\n",
    "        self.proba = pd.DataFrame(data=y_pred_proba, columns=self.model.classes_)\n",
    "        self.log.debug(f\"proba.shape {self.proba.shape}\")\n",
    "\n",
    "        def get_max_3_classes(row):\n",
    "            \"\"\"\n",
    "            Получает серию из предсказаний размерностью n-классов и возвращает три класса с максимальной вероятностью\n",
    "            и значания вероятности для этих классов.\n",
    "            \n",
    "            \"\"\"\n",
    "            ser = pd.Series(data=row.values, index=self.model.classes_)\n",
    "            ser.sort_values(inplace=True, ascending=False)\n",
    "            return ser[0:3].index[0],ser[0:3].values[0], \\\n",
    "                ser[0:3].index[1],ser[0:3].values[1], \\\n",
    "                ser[0:3].index[2],ser[0:3].values[2]\n",
    "\n",
    "        self.proba['top_1_class'], self.proba['top_1_proba'], \\\n",
    "            self.proba['top_2_class'], self.proba['top_2_proba'], \\\n",
    "            self.proba['top_3_class'], self.proba['top_3_proba'] = zip(*self.proba.apply(get_max_3_classes, axis=1))\n",
    "\n",
    "        self.proba['top_1_class'] = self.encoder_y.inverse_transform(self.proba['top_1_class'].values.ravel())\n",
    "        self.proba['top_2_class'] = self.encoder_y.inverse_transform(self.proba['top_2_class'].values.ravel())\n",
    "        self.proba['top_3_class'] = self.encoder_y.inverse_transform(self.proba['top_3_class'].values.ravel())\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\" Training on full data set \"\"\"\n",
    "        self.log.info(\"Final training...\")\n",
    "        self.get_encoded()\n",
    "        t0 = time()\n",
    "        self.model.fit(self.X_enc_train, self.y_enc_train)\n",
    "        self.log.debug(f\"Final training finished in {time() - t0:.3f} sec.\")\n",
    "\n",
    "    def get_report(self):\n",
    "        \"\"\" Prepare a new report \"\"\"\n",
    "\n",
    "        self.log.info(\"Calculate proba...\")\n",
    "        t0 = time()\n",
    "        # Generate proba\n",
    "        self.find_top_3(self.X_enc_pred)\n",
    "        X_pred = self.df[(self.df['isTrain']==False)& \\\n",
    "                         (~self.df['Last_Future_ship_to'] \\\n",
    "                          .isin(self.ships_to_exclude))][self.features]\n",
    "        X_pred.reset_index(inplace=True)\n",
    "        df_concat = pd.concat([X_pred['SWE_Store_Key'], self.proba.loc[:,'top_1_class':]], axis=1,join='inner')\n",
    "        df_info = self.df_codes.merge(right=df_concat,how='left',on='SWE_Store_Key')\n",
    "        df_info['SWE_Store_Key'] = df_info['SWE_Store_Key'].astype('str')\n",
    "        self.log.debug(f\"Done in {time() - t0:.3f} sec.\")\n",
    "\n",
    "        self.log.info(\"Open report...\")\n",
    "        t0 = time()\n",
    "        self.workbook = openpyxl.load_workbook(tf.report_file)\n",
    "        self.log.debug(f\"Done in {time() - t0:.3f} sec.\")\n",
    "        \n",
    "        self.log.info(\"Format report...\")\n",
    "        t0 = time()\n",
    "        worksheet = self.workbook['Sheet1']\n",
    "        rows = dataframe_to_rows(df_info, index=False, header=True)\n",
    "        proba_row = 2\n",
    "        proba_col = 54    # BB column\n",
    "        target_col = 51   # AY column\n",
    "        region_col = 49\n",
    "        name_col = 49\n",
    "        # Setup column width, setup title text, font and alignment\n",
    "        widths = [19,11,5,11,5,11,5]\n",
    "        captions = ['SWE Store Key','1 class',' 1 proba','2 class','2 proba','3 class','3 proba']\n",
    "        for i in range(7):\n",
    "            worksheet.column_dimensions[get_column_letter(proba_col + i)].width = widths[i]\n",
    "            cell = get_column_letter(proba_col + i) + str(proba_row)\n",
    "            worksheet[cell] = captions[i]\n",
    "            worksheet[cell].font = Font(name='Times New Roman', size=12, bold=True)\n",
    "            worksheet[cell].alignment = Alignment(horizontal='left', vertical='top')\n",
    "            worksheet[cell].fill = PatternFill(\"solid\", fgColor=\"00CCFFCC\")\n",
    "        # Go through all rows\n",
    "        for r_idx, row in enumerate(rows, proba_row):\n",
    "            # If proba is defined or the very first row with a title\n",
    "            if type(row[2])==float and not pd.isnull(row[2]):\n",
    "                # Go through columns in a row\n",
    "                for c_idx, value in enumerate(row, proba_col):\n",
    "                    worksheet.cell(row=r_idx, column=c_idx, value=value)\n",
    "                    worksheet.cell(row=r_idx, column=c_idx).font = Font(name='Arial', size=10, bold=False)\n",
    "                    worksheet.cell(row=r_idx, column=c_idx).alignment = Alignment(horizontal='left', vertical='top')\n",
    "                if r_idx > 2:\n",
    "                    proba = float(row[2])\n",
    "                    if proba >= 0.9:\n",
    "                        worksheet.cell(row=r_idx, column=target_col, value=row[1])\n",
    "                        worksheet.cell(row=r_idx, column=target_col).fill = PatternFill(\"solid\", fgColor=\"00CCFFCC\")\n",
    "                    elif proba >= 0.7:\n",
    "                        worksheet.cell(row=r_idx, column=target_col, value=row[1])\n",
    "                        worksheet.cell(row=r_idx, column=target_col).fill = PatternFill(\"solid\", fgColor=\"00FFCC99\")\n",
    "                    else:\n",
    "                        worksheet.cell(row=r_idx, column=target_col, value=row[1])\n",
    "                        worksheet.cell(row=r_idx, column=target_col).fill = PatternFill(\"solid\", fgColor=\"00FF9900\")\n",
    "        self.log.debug(f\"Done in {time() - t0:.3f} sec.\")\n",
    "\n",
    "    def save_report(self):\n",
    "        self.log.info(\"Save output file...\")\n",
    "        t0 = time()\n",
    "        self.workbook.save(self.output_file)\n",
    "        self.log.debug(f\"Saved in {time() - t0:.3f} sec.\")\n",
    "        self.log.info(f\"New report saved as '{self.output_file}''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.get_report()\n",
    "# tf.X_enc_pred\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "tf.log.info(\"Preparing report...\")\n",
    "# tf.find_top_3(tf.X_enc_pred)\n",
    "# print(tf.proba)\n",
    "\n",
    "X_pred = tf.df[(tf.df['isTrain']==False)& \\\n",
    "                             (~tf.df['Last_Future_ship_to'] \\\n",
    "                              .isin(tf.ships_to_exclude))][tf.features]\n",
    "# print(X_pred.shape, tf.X_enc_pred.shape)\n",
    "X_pred.reset_index(inplace=True)\n",
    "df_concat = pd.concat([X_pred['SWE_Store_Key'], tf.proba.loc[:,'top_1_class':]], axis=1,join='inner')\n",
    "\n",
    "df_info = tf.df_codes.merge(right=df_concat,how='left',on='SWE_Store_Key')\n",
    "# print(df_info[~df_info['top_1_class'].isna()])\n",
    "# del df_codes\n",
    "# del df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info['SWE_Store_Key'] = df_info['SWE_Store_Key'].astype('str')\n",
    "df_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "workbook = openpyxl.load_workbook(tf.report_file)\n",
    "worksheet = workbook['Sheet1']\n",
    "\n",
    "rows = dataframe_to_rows(df_info, index=False, header=True)\n",
    "\n",
    "proba_row = 2\n",
    "proba_col = 54    # BB column\n",
    "target_col = 51   # AY column\n",
    "region_col = 49\n",
    "name_col = 49\n",
    "\n",
    "# Setup column width, setup title text, font and alignment\n",
    "widths = [19,11,5,11,5,11,5]\n",
    "captions = ['SWE Store Key','1 class',' 1 proba','2 class','2 proba','3 class','3 proba']\n",
    "for i in range(7):\n",
    "    worksheet.column_dimensions[get_column_letter(proba_col + i)].width = widths[i]\n",
    "    cell = get_column_letter(proba_col + i) + str(proba_row)\n",
    "    worksheet[cell] = captions[i]\n",
    "    worksheet[cell].font = Font(name='Times New Roman', size=12, bold=True)\n",
    "    worksheet[cell].alignment = Alignment(horizontal='left', vertical='top')\n",
    "    worksheet[cell].fill = PatternFill(\"solid\", fgColor=\"00CCFFCC\")\n",
    "\n",
    "# Go through all rows\n",
    "for r_idx, row in enumerate(rows, proba_row):\n",
    "    # If proba is defined or the very first row with a title\n",
    "    if type(row[2])==float and not pd.isnull(row[2]):\n",
    "        # Go through columns in a row\n",
    "        for c_idx, value in enumerate(row, proba_col):\n",
    "            worksheet.cell(row=r_idx, column=c_idx, value=value)\n",
    "            worksheet.cell(row=r_idx, column=c_idx).font = Font(name='Arial', size=10, bold=False)\n",
    "            worksheet.cell(row=r_idx, column=c_idx).alignment = Alignment(horizontal='left', vertical='top')\n",
    "        if r_idx > 2:\n",
    "            proba = float(row[2])\n",
    "            if proba >= 0.9:\n",
    "                worksheet.cell(row=r_idx, column=target_col, value=row[1])\n",
    "                worksheet.cell(row=r_idx, column=target_col).fill = PatternFill(\"solid\", fgColor=\"00CCFFCC\")\n",
    "            elif proba >= 0.7:\n",
    "                worksheet.cell(row=r_idx, column=target_col, value=row[1])\n",
    "                worksheet.cell(row=r_idx, column=target_col).fill = PatternFill(\"solid\", fgColor=\"00FFCC99\")\n",
    "            else:\n",
    "                worksheet.cell(row=r_idx, column=target_col, value=row[1])\n",
    "                worksheet.cell(row=r_idx, column=target_col).fill = PatternFill(\"solid\", fgColor=\"00FF9900\")\n",
    "\n",
    "workbook.save(tf.output_file)\n",
    "# tf.log.debug(f\"Report prepared in {time() - t0:.3f} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.save_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-10 - 23:17:15 DEBUG: Input files were found\n",
      "2020-09-10 - 23:17:15 DEBUG: TerritoryFinder class initialized\n",
      "2020-09-10 - 23:17:15 INFO: Step 1 of 5: Loading and prepare data\n",
      "2020-09-10 - 23:17:15 INFO: Loading coordinates...\n",
      "2020-09-10 - 23:17:15 DEBUG: Rows in Coordinates.xlsx: 1000\n",
      "2020-09-10 - 23:17:15 INFO: Loading report file...\n",
      "2020-09-10 - 23:17:25 DEBUG: Rows in Report Territory Management_P10.xlsx: 4075\n",
      "2020-09-10 - 23:17:25 INFO: Merging territories with coordinates and start preprocessing...\n",
      "2020-09-10 - 23:17:25 INFO: Restore coordinates...\n",
      "2020-09-10 - 23:17:58 INFO: Step 2 of 5: Validate the model\n",
      "2020-09-10 - 23:17:58 DEBUG: Shapes: X (3860, 22), X_enc_train (3649, 22), X_enc_pred (211, 22)\n",
      "2020-09-10 - 23:17:58 DEBUG: Shape: y_enc_train (3649,)\n",
      "2020-09-10 - 23:17:58 DEBUG: Training finished in 0.122 sec.\n",
      "2020-09-10 - 23:17:59 INFO: Balanced accuracy score: 0.990\n",
      "2020-09-10 - 23:17:59 DEBUG: Validation finished in 0.1 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python376\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-10 - 23:18:01 INFO: Cross-validation average score: 0.985\n",
      "2020-09-10 - 23:18:01 DEBUG: Cross-validation finished in 2.281 sec.\n",
      "2020-09-10 - 23:18:01 DEBUG: proba.shape (1095, 13)\n",
      "2020-09-10 - 23:18:02 INFO: \n",
      "        Всего предсказаний: 1095\n",
      "        Правильных предсказаний: 99.6% (1091)\n",
      "        Предсказанных во втором варианте: 0.09% (1)\n",
      "        Предсказанных в третьем варианте: 0.000% (0)\n",
      "        Не предсказанных вообще 0.274% (3)\n",
      "        \n",
      "2020-09-10 - 23:18:02 INFO: \n",
      "    Интервал   Верных   Неверных   Нев./Общ.\n",
      "  (0.9, 1.0)     1073          1       0.09 | интервал (0.9, 1] кол. ошибок / общему кол. предсказаний = 0.09%\n",
      "  (0.8, 0.9)        8          0       0.00 | интервал (0.8, 1] кол. ошибок / общему кол. предсказаний = 0.09%\n",
      "  (0.7, 0.8)        0          0        nan | интервал (0.7, 1] кол. ошибок / общему кол. предсказаний = 0.09%\n",
      "  (0.6, 0.7)        6          0       0.00 | интервал (0.6, 1] кол. ошибок / общему кол. предсказаний = 0.09%\n",
      "  (0.5, 0.6)        2          0       0.00 | интервал (0.5, 1] кол. ошибок / общему кол. предсказаний = 0.09%\n",
      "  (0.4, 0.5)        2          1      33.33 | интервал (0.4, 1] кол. ошибок / общему кол. предсказаний = 0.18%\n",
      "  (0.3, 0.4)        0          0        nan | интервал (0.3, 1] кол. ошибок / общему кол. предсказаний = 0.18%\n",
      "  (0.2, 0.3)        0          2     100.00 | интервал (0.2, 1] кол. ошибок / общему кол. предсказаний = 0.37%\n",
      "\n",
      "2020-09-10 - 23:18:02 INFO: Step 3 of 5: Train the model\n",
      "2020-09-10 - 23:18:02 INFO: Final training...\n",
      "2020-09-10 - 23:18:02 DEBUG: Shapes: X (3860, 22), X_enc_train (3649, 22), X_enc_pred (211, 22)\n",
      "2020-09-10 - 23:18:02 DEBUG: Shape: y_enc_train (3649,)\n",
      "2020-09-10 - 23:18:03 DEBUG: Final training finished in 0.122 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pigidser\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:367: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-10 - 23:18:03 INFO: Step 4 of 5: Prepare report\n",
      "2020-09-10 - 23:18:03 INFO: Calculate proba...\n",
      "2020-09-10 - 23:18:03 DEBUG: proba.shape (211, 13)\n",
      "2020-09-10 - 23:18:03 DEBUG: Done in 0.426 sec.\n",
      "2020-09-10 - 23:18:03 INFO: Open report...\n",
      "2020-09-10 - 23:18:19 DEBUG: Done in 16.432 sec.\n",
      "2020-09-10 - 23:18:19 INFO: Format report...\n",
      "2020-09-10 - 23:18:20 DEBUG: Done in 0.173 sec.\n",
      "2020-09-10 - 23:18:20 INFO: Step 5 of 5: Save report\n",
      "2020-09-10 - 23:18:20 INFO: Save output file...\n",
      "2020-09-10 - 23:18:42 DEBUG: Saved in 22.394 sec.\n",
      "2020-09-10 - 23:18:42 INFO: New report saved as Report Territory Management_P10 Updated.xlsx\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import argparse\n",
    "# from territory_finder import TerritoryFinder\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Find a territory for a trade outlet\")\n",
    "\n",
    "# # Parse arguments\n",
    "# parser.add_argument(\"-c\", \"--coordinates\", type=str, action='store',\n",
    "#     help=\"Please specify the file with coordinates\", required=True)\n",
    "# parser.add_argument(\"-r\", \"--report\", type=str, action='store',\n",
    "#     help=\"Please specify the 'Territory Management Report' file\", required=True)\n",
    "# parser.add_argument(\"-o\", \"--output\", type=str, action='store',\n",
    "#     help=\"Please specify the file you wish to load weights from(for example saved.h5)\", required=False)\n",
    "# # parser.add_argument(\"-s\", \"--save\", type=str, action='store', help=\"Specify folder to render simulation of network in\", required=False)\n",
    "# # parser.add_argument(\"-x\", \"--statistics\", action='store_true', help=\"Specify to calculate statistics of network(such as average score on game)\", required=False)\n",
    "# # parser.add_argument(\"-v\", \"--view\", action='store_true', help=\"Display the network playing a game of space-invaders. Is overriden by the -s command\", required=False)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# # print(args.output)\n",
    "# # print(f\"{args.coordinates}, {args.report}, {new_report}\")\n",
    "\n",
    "# set an output file name\n",
    "# new_report = os.path.splitext(args.report)[0] + \" Updated\" if args.output == None else args.output\n",
    "\n",
    "tf = TerritoryFinder('Coordinates.xlsx', 'Report Territory Management_P10.xlsx',\n",
    "                                   'Report Territory Management_P10 Updated.xlsx')\n",
    "\n",
    "total_steps = 5\n",
    "\n",
    "tf.log.info(f\"Step 1 of {total_steps}: Loading and prepare data\")\n",
    "tf.load_data()\n",
    "\n",
    "tf.log.info(f\"Step 2 of {total_steps}: Validate the model\")\n",
    "tf.validate()\n",
    "\n",
    "tf.log.info(f\"Step 3 of {total_steps}: Train the model\")\n",
    "tf.fit()\n",
    "\n",
    "tf.log.info(f\"Step 4 of {total_steps}: Prepare report\")\n",
    "tf.get_report()\n",
    "\n",
    "tf.log.info(f\"Step 5 of {total_steps}: Save report\")\n",
    "tf.save_report()\n",
    "\n",
    "# print('Step 7 of 9: Define top 3 classes')\n",
    "\n",
    "# print(\"last line\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitf4bc0c7cfbbc4d93bdfa2658b00d8b91"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
